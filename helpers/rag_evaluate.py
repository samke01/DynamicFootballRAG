import pandas as pd

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

def evaluate_response_with_chat_openai(question, response, ground_truth, model="gpt-4o-mini"):
    """
    Evaluates the RAG response against the ground truth using LangChain's ChatOpenAI.

    Args:
        question (str): The original question asked.
        response (str): The response generated by the RAG pipeline.
        ground_truth (str): The expected correct answer.
        model (str): OpenAI model to use for evaluation (default: "gpt-4o-mini").
    Returns:
        bool: True if the response matches the ground truth, False otherwise.
    """
    # Initialize the ChatOpenAI model
    llm = ChatOpenAI(temperature = 0.0, model=model)
    
    # Define the system instruction and user input
    messages = [
        SystemMessage(content="You are a response evaluator. Compare the response with the ground truth and decide if they match semantically. Do not be very sensitive when it comes to minutes - e.g. 57:48 is the same as 58th minute. Only reply with 'True' if they match, or 'False' otherwise."),
        HumanMessage(content=(
            f"Question: {question}\n"
            f"Response: {response}\n"
            f"Ground Truth: {ground_truth}\n\n"
            "Does the response match the ground truth semantically? Reply with only 'True' or 'False'."
        ))
    ]

    try:
        # Use ChatOpenAI to evaluate
        result = llm(messages).content.strip()
        return result == "True"

    except Exception as e:
        print(f"Error in evaluation: {e}")
        return False
    
def evaluate_category(df, category, model="gpt-4o-mini"):
    """
    Evaluates the performance of RAG responses for a specific category.

    Args:
        df (pd.DataFrame): DataFrame with columns:
            - "question": The question string.
            - "response": The RAG-generated response.
            - "ground_truth": The correct answer.
        category (str): The category of the questions (e.g., "when", "who", "where").
        model (str): OpenAI model to use for evaluation (default: "gpt-4o-mini").

    Returns:
        dict: Performance metrics for the category.
    """
    
    # Evaluate each row
    results = []
    for index, row in df.iterrows():
        evaluation = evaluate_response_with_chat_openai(
            question=row["question"],
            response=row["response"],
            ground_truth=row["ground_truth"],
            model=model
        )
        results.append(evaluation)

    # Compute metrics for this category
    metrics = {
        "accuracy": accuracy_score([True] * len(results), results),
    }
    return metrics, results

def evaluate_all_data(dataframes, model="gpt-4o-mini"):
    """
    Concatenates all DataFrames, evaluates responses, and computes overall metrics.

    Args:
        dataframes (list of pd.DataFrame): List of DataFrames, each containing:
            - "question": The question string.
            - "response": The RAG-generated response.
            - "ground_truth": The correct answer.
        model (str): OpenAI model to use for evaluation (default: "gpt-4o-mini").

    Returns:
        dict: Overall performance metrics for the combined data.
    """
    # Concatenate all DataFrames
    all_data = pd.concat(dataframes, ignore_index=True)

    # Evaluate each row
    results = []
    for index, row in all_data.iterrows():
        evaluation = evaluate_response_with_chat_openai(
            question=row["question"],
            response=row["response"],
            ground_truth=row["ground_truth"],
            model=model
        )
        results.append(evaluation)

    # Compute overall metrics
    metrics = {
        "accuracy": accuracy_score([True] * len(results), results),
    }

    return metrics