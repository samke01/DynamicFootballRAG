{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace - Building all types of data\n",
    "\n",
    "This notebook was utilized to generate different types of analysis as well as datasets used for preprocessing. The sections are not in any particular order and were generated at different points in the project lifespan.\n",
    "\n",
    "The sections include:\n",
    "- Simplification of 'qualifiers' column - for simpler information extraction \n",
    "- Building a csv with all event types - for analysis and data understanding\n",
    "- Building a csv with all events - for analysis and data understanding\n",
    "- Building a dataset with all chunks - synthetically generated chunks were grouped into one chunk for the rag pipeline\n",
    "- Evaluating all features in datasets - for analysis and data understanding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from helpers.data_handlers import get_ordner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplification of 'qualifiers' column\n",
    "Modifies the 'qualifiers' column with the method 'simplify_qualifiers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:45<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from helpers.data_handlers import simplify_qualifiers\n",
    "\n",
    "# in this case the input ordner is the output ordner as well\n",
    "\n",
    "amount_gameweeks = 34\n",
    "type_data = \"EventData\"\n",
    "input_ordner = get_ordner(\"modified\")\n",
    "\n",
    "# Loop through each gameweek directory\n",
    "for gameweek in tqdm(range(1, amount_gameweeks + 1)):\n",
    "    \n",
    "    # Get the files in the directory\n",
    "    directory = os.listdir(input_ordner + str(gameweek))\n",
    "    \n",
    "    # Add events from all files to the list\n",
    "    for fname in directory:\n",
    "        \n",
    "        if type_data in fname:\n",
    "            \n",
    "            # Read the file\n",
    "            file_path = os.path.join(input_ordner + str(gameweek) + \"/\" + fname)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # events not to simlify as these are already in desired form\n",
    "            skip_events = ['MatchWon', 'MatchLost', 'MatchDraw']\n",
    "            # Simplify the qualifiers\n",
    "            df = simplify_qualifiers(df, skip_events)\n",
    "            \n",
    "            # Save the simplified DataFrame\n",
    "            df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a csv with all event types\n",
    "Generates a df that has all the event types - used to count the occurrence of each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_gameweeks = 34\n",
    "type_data = \"EventData\"\n",
    "input_ordner = get_ordner(\"modified\")\n",
    "\n",
    "# Create an empty list to store all event types\n",
    "all_events_type = []\n",
    "\n",
    "# Loop through each gameweek directory\n",
    "for gameweek in range(1, amount_gameweeks + 1):\n",
    "    \n",
    "    # Get the files in the directory\n",
    "    directory = os.listdir(input_ordner + str(gameweek))\n",
    "    \n",
    "    # Add events from all files to the list\n",
    "    for fname in directory:\n",
    "        \n",
    "        if type_data in fname:\n",
    "            \n",
    "            # Read the file\n",
    "            file_path = os.path.join(input_ordner + str(gameweek) + \"/\" + fname)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if 'type' in df.columns:\n",
    "                # Extend the list with the 'type' values\n",
    "                all_events_type.extend(df['type'].tolist())\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "all_events_df = pd.DataFrame(all_events_type, columns=['type'])\n",
    "all_events_df.value_counts().to_csv(\"data/Evaluation/EventTypesCount.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a csv with all events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "regenerate = True # TODO Set to True to regenerate the all_events.csv file\n",
    "amount_gameweeks = 34\n",
    "type_data = \"EventData\"\n",
    "input_ordner = get_ordner(\"modified\")\n",
    "\n",
    "if regenerate:\n",
    "    # Create an empty DataFrame to hold all events\n",
    "    all_events_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each gameweek directory\n",
    "    for gameweek in tqdm(range(1, amount_gameweeks + 1)):\n",
    "        # Get the directory path\n",
    "        directory_path = os.path.join(input_ordner + str(gameweek))\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        if os.path.exists(directory_path):\n",
    "            # Get all files in the directory\n",
    "            directory = os.listdir(directory_path)\n",
    "            \n",
    "            # Loop through the files\n",
    "            for fname in directory:\n",
    "                if type_data in fname:\n",
    "                    # Read the file and append it to the master DataFrame\n",
    "                    file_path = os.path.join(directory_path, fname)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        df['gameweek'] = gameweek  # Add a column to indicate the gameweek\n",
    "                        all_events_df = pd.concat([all_events_df, df], ignore_index=True)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Directory {directory_path} does not exist.\")\n",
    "            \n",
    "    all_events_df.to_csv(\"data/all_events.csv\", index=False) \n",
    "else:\n",
    "    all_events_df = pd.read_csv(\"data/all_events.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building one dataset with all chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:17<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries saved to data/event_chunks/chunks.json\n"
     ]
    }
   ],
   "source": [
    "# Number of gameweeks\n",
    "gameweeks = 34\n",
    "\n",
    "# Directory containing all the gameweeks where event data is modified to hold league standings as well as match outcomes\n",
    "input_dir = 'data/Bundesliga/modified/GW'\n",
    "\n",
    "# Initialize the list to hold the final data structure\n",
    "chunks = []\n",
    "\n",
    "# Loop through all the gameweeks\n",
    "for gw in tqdm(range(1, gameweeks + 1)):\n",
    "    # Get all the files in the directory\n",
    "    directory = os.listdir(input_dir + str(gw))\n",
    "\n",
    "    # Load the league standings for the current gameweek\n",
    "    league_standings_path = os.path.join(input_dir + str(gw), \"league_standings.csv\")\n",
    "    if os.path.exists(league_standings_path):\n",
    "        league_standings = pd.read_csv(league_standings_path)\n",
    "    else:\n",
    "        league_standings = pd.DataFrame()\n",
    "\n",
    "    # Loop through all the files in the directory\n",
    "    for fname in directory:\n",
    "        if \"EventData\" in fname:\n",
    "            # Load event and player data\n",
    "            event_df = pd.read_csv(os.path.join(input_dir + str(gw), fname))\n",
    "            player_df_path = os.path.join(input_dir + str(gw), fname.replace(\"EventData\", \"PlayerData\"))\n",
    "            if os.path.exists(player_df_path):\n",
    "                player_df = pd.read_csv(player_df_path)\n",
    "            else:\n",
    "                player_df = pd.DataFrame()\n",
    "\n",
    "            # Create chunks for each row in event data\n",
    "            for i, row in event_df.iterrows():\n",
    "                if row['type'] in ['FormationSet', 'End']:\n",
    "                    continue\n",
    "\n",
    "                entry = {\n",
    "                    \"metadata\": {\n",
    "                        'gameweek': gw,\n",
    "                        'event_id': row['eventId'],\n",
    "                        'team_id': row['teamId'],\n",
    "                        'player_id': row.get('playerId', None),\n",
    "                        'event_type': row['type']\n",
    "                    },\n",
    "                    \"content\": row.get('chunks', None)\n",
    "                }\n",
    "                chunks.append(entry)\n",
    "\n",
    "            # Create chunks for player data\n",
    "            for i, row in player_df.iterrows():\n",
    "                entry = {\n",
    "                    \"metadata\": {\n",
    "                        'gameweek': gw,\n",
    "                        'entity': 'PLAYER',\n",
    "                        'team_id': row['teamId'],\n",
    "                        'player_id': row['playerId'],\n",
    "                        'event_type': None\n",
    "                    },\n",
    "                    \"content\": row.get('chunks', None)\n",
    "                }\n",
    "                chunks.append(entry)\n",
    "\n",
    "    # Add league standings chunks\n",
    "    for i, row in league_standings.iterrows():\n",
    "        entry = {\n",
    "            \"metadata\": {\n",
    "                'gameweek': gw,\n",
    "                'entity': 'LEAGUE',\n",
    "                'team_id': row['teamId'],\n",
    "                'player_id': None,\n",
    "                'event_type': None\n",
    "            },\n",
    "            \"content\": row.get('chunks', None)\n",
    "        }\n",
    "        chunks.append(entry)\n",
    "\n",
    "# Save the chunks to a JSON file\n",
    "output_file = 'data/event_chunks/chunks.json'\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(chunks, json_file, indent=4)\n",
    "\n",
    "print(f\"Chunks saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating all features we have\n",
    "#### view results in data/Evaluation/EventData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_features(df):\n",
    "    \"\"\"\n",
    "    Evaluates the features in a given DataFrame by examining the number of unique values and the number of missing values.\n",
    "\n",
    "    Parameters:\n",
    "        df: DataFrame to evaluate.\n",
    "    Returns:\n",
    "        DataFrame that contains the unique value count and missing value count for each feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace empty lists with None values in the 'qualifiers' column\n",
    "    df.loc[df['qualifiers'].apply(lambda x: str(x) == '[]'), 'qualifiers'] = None\n",
    "    \n",
    "    feature_evaluation = pd.DataFrame({\n",
    "        'Unique Values': df.nunique(),\n",
    "        'Missing Values': df.isnull().sum(),\n",
    "        'Missing Percentage': df.isnull().mean() * 100\n",
    "    })\n",
    "    \n",
    "    return feature_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_directory = \"data/Evaluation/EventData\"\n",
    "    \n",
    "for event in all_events_df['type'].unique():\n",
    "    all_df = all_events_df[all_events_df['type'] == event]\n",
    "    evaluation = evaluate_features(all_df)\n",
    "    \n",
    "    relevant_columns = evaluation.index[evaluation['Missing Percentage'] < 30]\n",
    "    \n",
    "    text_directory = os.path.join(eval_directory, \"Text\")\n",
    "    if not os.path.exists(text_directory):\n",
    "        os.makedirs(text_directory)\n",
    "        \n",
    "    with open (os.path.join(text_directory, f\"{event}.txt\"), \"w\") as f:\n",
    "        f.write(f\"Event: {event}\\n\")\n",
    "        f.write(f\"This event occurred {len(all_df)} times.\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Relevant Columns: columns with less than 50% missing values:\\n\")\n",
    "        f.write(str(relevant_columns.values))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(evaluation.to_string())\n",
    "    \n",
    "    csv_directory = os.path.join(eval_directory, \"CSV\")\n",
    "    if not os.path.exists(csv_directory):\n",
    "        os.makedirs(csv_directory)\n",
    "        \n",
    "    evaluation.to_csv(os.path.join(csv_directory, f\"{event}.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wa-event-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
